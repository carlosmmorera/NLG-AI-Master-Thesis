\section{Generación de Lenguaje Natural}\label{s:nlg}
Como se ha descrito y observado gracias a la tabla \ref{tab:dailymail}, el tráfico de correos electrónicos diarios continúa en constante crecimiento hasta llegar, al menos, a la gigantesca cifra de 376 mil millones enviados al día en todo el mundo. Esto se revierte en una gran cantidad de tiempo invertido para la redacción de todos estos mensajes que no se mandan de manera automática. Sin embargo, esta gran dedicación al e-mail lleva produciéndose desde hace más de una década, cuando no nos encontrábamos con cifras de tráfico tan elevadas. Según \cite{mckinsey}, de media los empleados invertían el 28\% de su tiempo semanal en la gestión del correo electrónico (como viene reflejado en la figura \ref{fig:e-mailwork}). Esto se traduce en más de once horas dedicadas única y exclusivamente a leer y contestar mensajes, enviando y recibiendo una media de 124 e-mails por día \citep{radicati2015email}. Por si estos datos no fueran suficientemente preocupantes, de cara a la productividad laboral y resolución eficiente de las tareas, según \cite{forbes} este problema se ha agravado en los últimos años por diversas causas (entre las que se encuentra la pandemia de la Covid-19). En definitiva, hoy en día podemos afirmar que tanto en el ámbito profesional como personal se invierte una gran cantidad de esfuerzo y tiempo para gestionar nuestra cuenta de e-mail, lo cual plantea un problema en el que vemos que, en lugar de ser una herramienta útil, se convierte en una responsabilidad más que debe llevarse al día y de la que no es posible desprenderse ya que la capacidad de mandar estos mensajes es imprescindible para llevar a cabo tareas del día a día. Pero, ¿y si fuera posible ahorrar todo este tiempo de escritura de correos electrónicos?

\begin{figure}[h]
	\centering%
	\centerline{\includegraphics[width = 0.9\textwidth]{Imagenes/Bitmap/mckinsey.png}}%
	\caption{Porcentaje de tiempo de un trabajador dedicado a cada tarea}%
	\label{fig:e-mailwork}
\end{figure}

Para lograr este propósito es imprescindible profundizar en la rama de la Inteligencia Artificial conocida como \textit{Generación de Lenguaje Natural} (cuyas siglas son \textit{NLG} por su nombre en inglés \textit{Natural Language Generation}). Un buen ejemplo de aplicación de las técnicas de generación automática de textos son los 100.000 libros que Philip M. Parker puso a la venta en la plataforma \textit{Amazon.com} incluyendo títulos de temáticas tan variadas como \textit{El libro oficial del paciente sobre la estenosis espinal} \citep{parker2002official}, \textit{Perspectivas mundiales de 2009 a 2014 de los envases de 60 miligramos de Fromage Frais} \citep{parkerfromage},  \textit{Perspectivas de 2007 a 2012 de las tapetes de nudo, alfombras de baño y conjuntos que miden 6 pies por 9 pies o menos en la India} \citep{parkerrugs} y \textit{Tesauro Quechua - Inglés} \citep{parkerquechua}.

Resulta evidente que dicha cantidad de libros no pudieron ser escritos por Parker, sino que debió hacerse uso de técnicas de generación automática de textos. El algoritmo utilizado para dicho propósito, se engloba dentro de los métodos de generación conocidos como \textit{text-to-text} (texto a texto en castellano), dado que este tipo de técnicas toman como entrada textos ya existentes (normalmente escritos a mano y no generados automáticamente) y producen un nuevo texto coherente como salida. Otras aplicaciones de este tipo de métodos son la traducción automática de un idioma a otro \citep{hutchins2009introduction, oettinger2013automatic}, el resumen automático de textos \citep{mani2001automatic, nenkova2011automatic}, la simplificación de textos complejos, ya sea para hacerlos más accesibles para un público de lectores de bajo nivel de alfabetización \citep{siddharthan2014survey, bautista2011empirical} o niños \citep{macdonald2016summarising}, corrección automática de ortografía, gramática y texto \citep{kukich1992techniques, ng2014conll}, generación automática de revisiones de artículos científicos \citep{bartoli2016your}, generación de paráfrasis dada una frase de entrada \citep{bannard2005paraphrasing}, generación automática de preguntas con fines didácticos y educativos \citep{brown2005automatic}, generación automática de relatos dada una descripción conceptual de la historia deseada \citep{gervas2004story} o reescritura de textos (en concreto correos electrónicos) con estilo en función del destinatario \citep{mitfg}.

Además de estos métodos text-to-text, existen los llamados \textit{data-to-text} (datos a texto), en los cuales en lugar de recibir un texto como entrada, se genera el lenguaje a partir de datos. Estos pueden ser de todo tipo para dar lugar a informes o resúmenes como pueden ser de índole climatológica \citep{goldberg1994using, ramos2014linguistic}, financiera \citep{plachouras2016interacting}, ingenieril, como por ejemplo el trabajo desarrollado por \cite{yu2007choosing} para generar resúmenes de datos recopilados por sensores en turbinas de gas, sanitaria \citep{huske2003text, banaee2013towards}, como la investigación llevada a cabo por \cite{portet2009automatic} para obtener informes textuales a partir de datos de cuidados intensivos neonatales, o, incluso, deportivos \citep{theune2001data, chen2008learning}. Además de informes o resúmenes, también se utilizan los métodos \textit{data-to-text} para otros propósitos como la composición de discursos narrativos para relatos de varios personajes a partir de partidas de ajedrez \citep{gervas2014composing}, redacción de periódicos electrónicos a partir de datos de sensores \citep{molina2011generating}, generación de texto que aborda problemas medioambientales como el seguimiento de la fauna \citep{siddharthan2012blogging, ponnamperuma2013tag2blog}, la información medioambiental personalizada \citep{wanner2015getting} y la mejora del compromiso de los ciudadanos científicos a través de los comentarios generados \citep{van2016role} o producción de información interactiva sobre artefactos culturales \citep{stock2007adaptive}, entre otros.

Debido a que el objetivo de este trabajo se centra en la generación de correos electrónicos a partir del asunto, exploraremos en detalle las técnicas de Generación de Lenguaje Natural y, en especial, los métodos text-to-text. Para profundizar en los algoritmos y arquitecturas empleados ante los problemas de tipo data-to-text, conviene consultar la investigación llevada a cabo por \cite{gatt2018survey}, en la cual muestran el estado del arte de los trabajos realizados en este ámbito.

\subsection{¿Qué es la Generación de Lenguaje Natural?}
Dado que tanto los sistemas text-to-text como data-to-text y todas sus aplicaciones mencionadas anteriormente pertenecen a la rama de Generación de Lenguaje Natural, esta no debe definirse en función de la entrada del sistema, sino en la salida. Según \cite{biblia} la NLG es la conceptualización del ``campo de la inteligencia artificial y la lingüística computacional que se centra en los sistemas informáticos que son capaces de producir textos comprensibles en inglés u otra lengua humana. [...] Como área de investigación, la NLG presenta una perspectiva única ante problemas fundamentales de la inteligencia artificial, la ciencia cognitiva y la interacción. Estos incluyen cuestiones como por ejemplo cómo deben ser representados y cómo debe razonarse con la lingüística y el dominio del conocimiento, qué significa que un texto esté correctamente redactado y cómo es la mejor forma de comunicar información entre las computadoras y los usuarios.'' Por lo tanto, la Generación de Lenguaje Natural se puede definir como el ámbito que engloba el estudio de la producción de lenguaje no artificial, así como el diseño e implementación de algoritmos y sistemas computacionales cuyo resultado debe ser un texto que imite la forma en que los humanos se comunican verbalmente \citep{vicente2015generacion}, ya sea oralmente o por escrito \citep{del2007que}. Es decir, independientemente de la entrada recibida, se precisa el significado de NLG a partir de la salida esperada por el problema planteado. Tanto es así, que, como hemos visto, la entrada del sistema puede variar excesivamente \citep{mcdonald1993issues}: desde textos (que son precisamente los sistemas text-to-text) hasta datos de todo tipo como partidas de ajedrez \citep{gervas2014composing}, pictogramas \citep{gonzalez2019traductor} e, incluso, vídeos \citep{thomason2014integrating}. Sin embargo, autores como \cite{duvsek2020evaluating} acotan la definición de los sistemas de NLG estableciendo que la entrada deben ser representaciones semánticas, obviando así la primera tarea de la arquitectura propuesta por \cite{biblia} conocida como macro planificación o determinación del contenido (se explicará en la sección \ref{sss:realizer}), que es precisamente el punto en el que se generan dichas representaciones semánticas.

Cabe destacar que, aunque desde un principio hayamos diferenciado entre métodos text-to-text y data-to-text, ni los límites entre las dos aproximaciones ni la pertenencia de algunas técnicas a ellas se encuentran claramente definidos. Un ejemplo de ello podemos encontrarlo en la generación automática de resúmenes de textos. En principio se caracterizaría claramente como un sistema text-to-text. No obstante, al hacer frente a este problema se han desarrollado soluciones con las conocidas técnicas abstractivas \citep{genest2011framework}, que, como explican \cite{hahn2000challenges}, a diferencia de los métodos de extracción evitan recoger las frases completas y se limitan a tomar unidades semánticas. Este tipo de técnicas usadas, por ejemplo, en la obtención de opiniones de reseñas para la posterior generación de frases nuevas \citep{labbe2012towards}, también provienen de problemas data-to-text. A la inversa, un sistema data-to-text puede hacer uso de técnicas que principalmente son utilizadas en los casos de uso text-to-text \citep{mcintyre2009learning, kondadadi2013statistical}. Por otro lado, podría parecer que los métodos de \textit{deep learning} \citep{goodfellow2016deep} deben ser mayoritariamente utilizados en los problemas data-to-text utilizando el trabajo llevado a cabo por \cite{mikolov2013efficient}. Sin embargo, se han desarrollado extensamente esta clase de soluciones para la NLG con gran variedad de arquitecturas como las redes neuronales recurrentes \citep{cho2014learning, tang2016context}, muy a menudo combinadas con la memoria a corto plazo o LSTM \citep{chen2016enhanced}, o las arquitecturas conocidas como \textit{transformers} \citep{transformers}.

\subsection{Arquitecturas para la Generación de Lenguaje Natural}
Ante el problema de generación de lenguaje natural, se han propuesta diversas soluciones para abordarlo. Sin embargo, actualmente preponderan dos tipos de arquitecturas: la propuesta por \cite{biblia}, también conocida como arquitectura \textit{realizer} (toma el nombre de una de sus fases), que divide la generación en distintas subtareas y las aborda por separado, y la presentada por \cite{transformers}, también conocida como arquitectura \textit{transformer}, que expone una arquitectura constituida mayoritariamente por redes neuronales. Aunque en este trabajo hayamos hecho uso de esta última debido a la dificultad de establecer un dominio de lenguaje en los correos electrónicos, a continuación se hará una breve introducción a ambas.

\subsubsection{Arquitectura realizer}\label{sss:realizer}
A pesar de que en el trabajo de \cite{biblia} se centren principalmente en los sistemas de generación de lenguaje natural de tipo \textit{data-to-text}, esta arquitectura también ha sido utilizada por sistemas \textit{text-to-text} como para la generación automática de resúmenes a través de métodos abstractivos \citep{genest2011framework}. De hecho, incluso es posible hacer uso de la conceptualización de la entrada del sistema planteada por \cite{biblia} para este tipo de problemas.

La filosofía de esta arquitectura se centra en la modularización de las diferentes tareas a abordar durante la generación de lenguaje natural. De esta manera, cada módulo se enfrenta a un reto específico con el que debe lidiar y se conecta con el módulo anterior haciendo coincidir la salida del previo con la entrada del actual y, lo mismo, con el módulo posterior. Asimismo, se construye un \textit{pipeline} o arquitectura en secuencia de tareas con cada uno de los módulos.

La entrada de esta arquitectura viene determinada por una tupla de cuatro elementos $(k, c, u, d)$ donde cada uno de ellos puede representarse de distintas maneras. El primer elemento de la tupla es la \textit{base de conocimiento} (la letra viene dada por su denominación ingles \textit{knowledge source}). Se trata de la información acerca del dominio de nuestro sistema de generación de lenguaje natural, la cual suele consistir en un conjunto de bases de datos y bases de conocimiento, como las ontologías \citep{fensel2001ontologies}, que nuestra aplicación puede consultar durante su ejecución. Tanto la representación como el contenido de la base de conocimiento son altamente dependientes del tipo de aplicación que queramos construir, por ejemplo, en el trabajo de \cite{reiter2005choosing} la base de conocimiento consiste en parámetros meteorológicos numéricos de un modelo de predicción NWP, mientras que en el desarrollo presentado por \cite{reiter1995automatic} se utiliza como base de conocimiento un sistema de representación de conocimiento del mismo tipo que KL-ONE \citep{brachman1989overview} estructurado de manera jerárquica mediante relaciones \textit{is-a} y \textit{part-of}, del cual se pueden extraer diversas propiedades de las entidades que lo componen. Precisamente esta gran variabilidad en tan solo la primera componente de la entrada de la arquitectura realizer, es lo que sustenta la afirmación de \cite{biblia} de que no es posible proporcionar una caracterización formal genérica de lo que es una base de conocimiento y dificulta el establecimiento de esta componente en el sistema desarrollado si se hubiera elegido esta opción de arquitectura, ya que los correos electrónicos versan de una amplia variedad de temáticas muy distintas.

La segunda componente de la tupla de entrada es el objetivo de la comunicación (en inglés \textit{communicative goal}). Este describe el propósito del texto para el que se quiere generarlo. Es importante no confundirlo con el propósito general del sistema de generación de lenguaje natural. Por ejemplo, el objetivo final de sistema implementado por \cite{turner2007selecting} es generar resúmenes textuales de datos de predicción meteorológica numérica espacio-temporal. Sin embargo, su propósito comunicativo de una ejecución determinada es el de presentar predicciones meteorológicas de una localización geográfica y un momento temporal dados.

La letra u de la tupla se corresponde con el modelo de usuario (en inglés \textit{user model}), el cual consiste en una caracterización del receptor o público objetivo al que va dirigido el texto generado. Al igual que con el propósito comunicativo, no debe confundirse con los espectadores del sistema. Por ejemplo, el sistema implementado por \cite{reiter1999types} tiene como objetivo dirigirse a personas que consumen tabaco, ya que trata de generar cartas que convenzan a los pacientes con este hábito para que intenten superar su adicción. No obstante, no resulta adecuado utilizar el mismo tipo de técnicas de convicción y el mismo lenguaje para una persona que ha comenzado a fumar desde hace poco que a otra que lleva muchos años consumiendo tabaco. También, podría resultar interesante definir perfiles en función de la edad y otras características del paciente que permitirían personalizar aún más estas cartas. Son precisamente este tipo de propiedades las que englobaría el modelo de usuario que el sistema toma como entrada. No obstante, a pesar de que no existen demasiados ejemplos de trabajos que incluyan esta variable de entrada como lo desarrollan en su estudio \cite{goldberg1994fog}, por la dificultad de variación de los textos en función de dichos perfiles, este problema se suele tratar de abordar a través del estudio de la estilometría \citep{mitfg}.

El último componente de la entrada es la historia del discurso (en inglés \textit{discourse history}), la cual consiste en un modelo de la información transmitida y los temas tratados en el texto producido hasta el momento de la ejecución del sistema. Esto permite a la aplicación conocer las entidades y propiedades ya mencionadas gracias a las cuales es posible hacer un uso adecuado de los recursos anafóricos como los pronombres. En los sistemas de generación de lenguaje natural de interacción única, es decir, aquellos cuya ejecución produce un único texto con independencia de los generados en ejecuciones previas, la historia discursiva comienza como una estructura de datos vacía y se construye y utiliza durante la redacción del texto a generar. Todo lo contrario son los sistemas de diálogo, como los \textit{chatbots}, en los cuales la historia del discurso suele hacer referencia al registro de diálogo, utilizado como repositorio de información sobre las interacciones previas entre el usuario y la aplicación de NLG. En trabajos como el desarrollado por \cite{milosavljevic1996text}, en el cual se espera que los usuarios interactúen con una serie de textos relacionados, la historia discursiva resulta ser una mezcla de la utilizada en los sistemas de interacción única con la preponderante en los sistemas de diálogo. De esta manera, esta última componente de la tupla de entrada facilita el hacer referencias en el nuevo texto a entidades o conceptos mencionados en el texto actual o previos, o utilizar marcadores discursivos como ``como se ha mencionado anteriormente'' en momentos en que el texto generado repite información ya presentada en los anteriores.

Aunque puede resultar obvio, es importante tener en cuenta que, en la mayoría de los casos, será necesario un preprocesado del texto o los datos de entrada (sobre todo si han de ser analizados e interpretados) para facilitar el trabajo a la arquitectura presentada por \cite{biblia}, de manera que sea más sencillo utilizar la entrada en cada una de las fases \citep{han2011data}. Este primer preprocesado es plenamente opcional y depende del origen de la entrada que se vaya a utilizar.

Una vez se conoce la entrada y salida establecida para un sistema de generación de lenguaje natural, se puede comenzar la presentación de la arquitectura modular secuencial. Según \cite{biblia}, el proceso de generación puede descomponerse en tres fases: la macro planificación, la micro planificación y la realización (que da nombre a la arquitectura por ser la última fase). Los módulos que implementan cada una de ellas se conectan entre sí como muestra la figura \ref{fig:pipelinenlg}.

\begin{figure}[h]
	\centering%
	\centerline{\includegraphics[width = \textwidth]{Imagenes/Bitmap/pipeline-nlg.jpg}}%
	\caption{Arquitectura modular secuencial propuesta por \cite{biblia} para la NLG}%
	\label{fig:pipelinenlg}
\end{figure}

En términos generales, el trabajo del módulo de macro planificación es producir una especificación del contenido del texto y su estructura, mediante el uso del dominio y el conocimiento de la aplicación sobre qué información es la más adecuada teniendo en cuenta la tupla de entrada. Esta fase normalmente también requiere conocer cómo suelen estructurarse los documentos del dominio de nuestro sistema. Muchas de las técnicas empleadas para la implementación de la macro planificación suelen asemejarse a las utilizadas en el ámbito de los sistemas expertos.

Para que el texto sea coherente, es preciso estructurar el contenido del mismo en el orden correcto. Por este motivo, la salida del módulo de macro planificación, el plan del documento, suele implementarse como una estructura de datos, generalmente arbórea, donde en cada nodo se encapsula la información más importante que debe formar parte de un párrafo o frase, además de información de cómo se relaciona con el resto de nodos. Con el objetivo de generar esta salida, se dividen las tareas de este nodo en determinación del contenido y estructuración del documento.

La determinación del contenido es la primera tarea de la macro planificación y, por tanto, del proceso de generación. En ella el sistema decide qué información es relevante para ser incluida en el texto y cuál no. Por lo general, en los sistemas data-to-text, se puede extraer más información en los datos de la que se quiere transmitir y, por ese motivo, cobra importancia la capacidad de selección y existen numerosas investigaciones al respecto, como la de \cite{yu2007choosing}. Aunque la determinación del contenido está presente en la mayoría de los sistemas de generación de lenguaje natural \citep{mellish2006reference}, los enfoques suelen estar estrechamente relacionados con el dominio, de forma que se construye una estructura de datos ad hoc, cuyos elementos se denominan ``mensajes'' o también son llamadas ``representaciones semánticas'' por autores como \cite{duvsek2020evaluating}. Dicha estructura de datos se crea con el fin de especificar la salida de esta tarea (un ejemplo conceptual de un mensaje para un sistema que genera textos sobre la situación meteorológica puede observarse en la figura \ref{fig:messagenlg}). Este planteamiento de la determinación del contenido, dificulta el desarrollo de sistemas en los que el dominio es extenso, como es el caso de los correos electrónicos, ya que no es posible implementar una clase que englobe todas las casuísticas (como se verá en la sección \textcolor{red}{poner nº}, para resolver este problema se utilizará el concepto de \textit{Information Items} el cual pertenece al ámbito del resumen automático de textos). Sin embargo, sí se ha tratado de encontrar otro tipo de definición a la salida de esta tarea, como ocurre en el trabajo de \cite{guhe2007incremental}, el cual presenta una explicación cognitivamente plausible e incremental de la determinación del contenido, basada en estudios sobre las descripciones de observadores de eventos dinámicos a medida que se desarrollan.

\begin{figure}[h]
	\centering%
	\centerline{\includegraphics[width = 0.7\textwidth]{Imagenes/Bitmap/message-nlg.jpg}}%
	\caption{Ejemplo de mensaje}%
	Imagen extraída de \cite{biblia}
	\label{fig:messagenlg}
\end{figure}

La elección del contenido que debe ser expresado en el texto depende de varios factores, entre los que se encuentran: el propósito comunicativo, el modelo de usuario, las restricciones de la salida por limitaciones del dominio (como la longitud máxima que debe tener un texto) y la fuente de información (o base de conocimiento) subyacente disponible (es importante, aunque parezca evidente, no pretender generar información sobre la que el sistema no posee conocimiento o no es capaz de deducirlo con sus módulos de razonamiento correspondientes).

Como se ha mencionado anteriormente, además de la determinación del contenido, el módulo de macro planificación se encarga de abordar la tarea de estructuración del documento. Esta engloba todas las decisiones relativas a cómo cada parte del contenido debe ser agrupada en el texto y cómo debe ser relatada en términos retóricos. La estructura subyacente de un texto puede ser vista de manera jerárquica con los elementos textuales como cláusulas y las frases como constituyentes de largos fragmentos de texto, que son a su vez elementos de otros fragmentos más largos. Un texto puede ser analizado como una estructura arbórea cuyos elementos vienen relacionados por el tema que tratan (todas las frases sobre una misma temática podrían situarse en el mismo párrafo) y las relaciones discursivas (por ejemplo, es posible pasar de hablar en términos generales a entrar en detalles). Para abordar esta tarea la mayoría de trabajos, como el de \cite{williams2008generating}, suelen hacer uso de la llamada teoría de la estructura retórica \citep{mann1987rhetorical}, cuyas siglas son RST por su nombre en inglés \textit{Rhetorical Structure Theory}. La idea básica subyacente de la RST es que la coherencia y cohesión vienen dadas en virtud de las relaciones que mantienen los elementos del texto. También se pueden encontrar investigaciones, como es el caso de la realizada por \cite{mckeown1992text}, que lo resuelven mediante esquemas, es decir, patrones que determinan cómo se debe construir un documento a partir de elementos constituyentes que pueden ser mensajes o instancias de otros esquemas. Además, permiten incluir elementos opcionales que solo se incluirían cuando se cumplen ciertas condiciones de activación. Un ejemplo de implementación de los esquemas son las gramáticas independientes del contexto \citep{cremers1975context}.

En definitiva, el primer módulo de la arquitectura realizer es el de macro planificación, el cual construye primero los mensajes (seleccionando los datos, resumiéndolos si fuera necesario, razonando sobre ellos y adaptándolos en función de la tupla de entrada) y, a continuación, define una relación entre todos ellos (normalmente jerárquica), generando así lo que se conoce como plan del documento.

El plan del documento es la entrada del módulo de micro planificación, que es el encargado de las tareas de agregación (decidir cómo el plan del documento se convertirá en estructuras lingüísticas como frases y párrafos, y el orden de las mismas si esto no se ha resuelto en la tarea de estructuración del documento), lexicalización (decidir qué recursos lingüísticos, como palabras o construcciones sintácticas, serán utilizados) y generación de expresiones referenciales (decidir qué expresiones serán utilizadas para las distintas entidades). Por ejemplo, para un sistema de generación de reportes meteorológicos, el plan de documento podría especificar que en el texto debería incluirse el hecho de que en febrero de 2020 se observaron temperaturas por debajo de la media. No obstante, la decisión de como referirse a febrero de 2020 (existen numerosas opciones como ``el mes'', ``febrero'', ``febrero de 2020'', ``el segundo mes del año'', etcétera) es tarea del módulo de micro planificación. O también la salida de  la macro planificación puede añadir, además de la bajada media de temperaturas, un descenso en el mismo mes en la media de precipitaciones. Será entonces el módulo de micro planificación el encargado de determinar si estos dos hechos deben expresarse en frases separadas o combinados en una misma utilizando, quizás, la estructura de una oración coordinada copulativa.

Resulta razonable pensar que no cada mensaje generado en el plan del documento necesita ser expresado en una frase separado del resto. El hecho de combinar varios mensajes en una misma oración probablemente produzca un texto más fluido y legible \citep{dalianis1999aggregation, cheng2000capturing}. Precisamente esta decisión es la que se debe tomar en la fase de agregación, junto con la transformación de los mensajes en estructuras lingüísticas y elementos textuales más amplios como pueden ser los párrafos. Como explican \cite{biblia}, esta tarea no necesariamente precede a la de lexicalización, sino que suelen implementarse de manera que se ejecuten ambas en paralelo interactuando una con la otra, ya que suelen estar fuertemente relacionadas la elección de los términos a utilizar (lexicalización) y las estructuras lingüísticas empleadas.

En cuanto a la definición técnica de la agregación, esta tarea se ha interpretado de distintas maneras, desde la eliminación de redundancias hasta la combinación de las estructuras lingüísticas. Tanto es así, que \cite{reape1999just} hacen una distinción entre agregación a un nivel semántico y a nivel sintáctico. Esta fase se ha desarrollado ampliamente centrándose en las restricciones y reglas específicas del dominio de la aplicación, como es el caso de las investigaciones de \cite{hovy1987generating} y \cite{shaw1998clause}, aunque también se han implementado aplicaciones utilizando métodos dirigidos por corpus de ejemplos en los que las reglas se adquieren a partir de estos datos \citep{walker2001spot, stent2004trainable}.

La lexicalización, tarea del módulo de micro planificación, consiste en la elección de las distintas palabras (nombres, verbos, adjetivos y adverbios) que se requieren para expresar los distintos mensajes del plan del documento. La complejidad de este proceso radica en la cantidad de alternativas que el sistema de generación de lenguaje natural puede contemplar. A menudo, las restricciones contextuales (como intentar evitar la repetición de ideas o entidades) pueden ser de gran ayuda. Si el objetivo es generar textos con cierta variación \citep{theune2001data}, una solución puede ser elegir de manera aleatoria la opción de lexicalización. Sin embargo, puede ocurrir que aparezcan restricciones estilísticas o haya que tener en cuenta otras consideraciones como la actitud o postura afectiva acerca de lo que se está redactando \citep[Sección 5]{fleischman2002towards}. Este tipo de limitaciones dependerán del dominio de la aplicación.

Por otro lado, la lexicalización podría complicarse por dos razones \citep{bangalore2000corpus}: puede suponer la elección entre palabras de semántica similar, sinónimo o taxonómicamente relacionadas \citep{stede2000hyperonym, edmonds2002near}, y no siempre es sencillo modelar la lexicalización en términos de un mapeo nítido entre conceptos y palabras. Una fuente de dificultad es la imprecisión e inexactitud del lenguaje, que surge, por ejemplo, con términos que denotan propiedades que son graduables. Un buen ejemplo de ello es al seleccionar adjetivos como ancho o alto basado en las dimensiones de una entidad. Esto requiere que el sistema razone sobre la anchura o altura de objetos similares, pudiendo utilizar un estándar de comparación \citep{van2012not, kennedy2004scale}.

La otra tarea de la que se encarga el módulo de micro planificación es la generación de expresiones referenciales, la cual consiste en redactar expresiones que se refieran a las distintas entidades del texto que permitan al receptor identificar dicha entidad dado el contexto. Resulta un reto en la generación de lenguaje natural ya que hay distintas maneras de generar estas referencias, tanto la primera vez que se menciona la entidad (referencia inicial) como cuando esta es nombrada después de haber sido introducida en el discurso (referencia subsiguiente). Esta descripción de la generación de expresiones referenciales, puede parecer que guarda varias similitudes con la lexicalización, pero \cite{biblia} indican que la diferencia esencial entre ambas tareas es que, la que ahora atañe al texto, es ``una tarea de discriminación en la cual el sistema necesita comunicar suficiente información de cara a distinguir una entidad del dominio frente al resto de entidades'' evitando la ambigüedad. Esta fase de la micro planificación ha sido muy llamativa para varios investigadores como \cite{siddharthan2011information}, quienes han centrado sus esfuerzos en presentar diversas soluciones que permitan la implementación de esta tarea. La primera aproximación que se nos puede ocurrir es la llamada forma referencial, en la cual las entidades son descritas mediante pronombres, nombres propios o una descripción concreta. Esta elección depende, en cierta forma, en la medida en que la entidad se encuentre en el foco de atención de la estructura lingüística o se quiera destacar \citep{poesio2004centering}. De hecho, tales nociones subyacen a muchas investigaciones acerca de la generación de pronombres \citep{mccoy1999generating, callaway2002narrative, kibble2004optimizing}.

Precisamente elegir la forma referencial ha sido objeto de diversos trabajos, como el de \cite{belz2009generating}, sobre la generación de expresiones referenciales aprovechando el contexto que engloba lo que se quiere redactar, y la investigación de \cite{ferreira2017linguistic} que pone de manifiesto la problemática en la generación de nombres propios para las entidades.

Como puede observarse en la figura \ref{fig:pipelinenlg}, la salida del módulo de micro planificación es la especificación del texto. Se trata de un objeto que proporciona una definición completa y precisa del documento a generar. Análogo al plan de documento, la especificación del texto suele implementarse con una estructura arbórea en la cual las hojas son los objetos que \cite{biblia} llama especificación de oración (las cuales pueden conceptualizarse de distintas formas: fragmento de texto enlatado, estructura sintáctica abstracta o un frame de casos lexicalizados), mientras que los nodos internos suelen contener la información acerca de la estructura del documento en términos de párrafos, secciones, etcétera. La estructura que describe la especificación del texto es la estructura lógica del documento, no la física (cómo el material es distribuido a lo largo de las páginas, columnas, líneas), la cual es tarea del último módulo de la arquitectura realizer. En esta salida del módulo de micro planificación, no se suelen encontrar relaciones discursivas, los nodos internos normalmente no las especifican. Si fuera necesario comunicarlas, el módulo de micro planificación llevaría a cabo los cambios convenientes a nivel de especificación de oración. Otro dato importante de esta estructura de datos es que el orden de los constituyentes (los nodos hijos) está determinado por cómo se recorrerá el árbol de la especificación del texto.

Por último, el módulo de realización (por el cual se le da nombre a la arquitectura y es el que producirá el texto final) llevará a cabo las tareas de realización lingüística y realización estructural. La primera consiste en recibir las representaciones abstractas de las frases (especificación de oración), producidas en la micro planificación, y convertirlas en texto. Al igual que los documentos no son secuencias de oraciones ordenadas de forma aleatoria, las frases no son secuencias de palabras ordenadas al azar. Cada idioma se define, al menos en parte, por una serie de reglas que determinan cuándo una sentencia está bien construida. Dichas reglas son dirigidas por el campo de la morfología y la sintaxis. La realización lingüística suele definirse como el problema de aplicar reglas gramaticales a representaciones abstractas con el fin de producir un texto correcto a nivel sintáctico y morfológico. La complejidad de esta tarea dependerá entonces de cuán distinta sea la representación abstracta de su redacción final.

Para resolver el problema de realización lingüística, en el cual se debe ordenar y especificar los distintos constituyentes de una frase así como generar su correcta forma morfológica (incluyendo conjugación de verbos y concordancia de persona, género y número de las distintas entidades lingüísticas); se han propuesto distintas aproximaciones: plantillas preconstruidas, sistemas basados en la gramática preconstruidos y métodos estadísticos. La primera aproximación suele resultar de utilidad cuando el dominio de la aplicación es pequeño y la variación esperada es ínfima \citep{mcroy2003augmented}. Consiste en oraciones construidas previamente en las cuales solo es necesario rellenar ciertas entidades y ajustar algunas características morfológicas. La ventaja de las plantillas es que se tiene un control absoluto sobre la calidad de la salida y evita la generación de estructuras gramaticalmente incorrectas. Además, pueden utilizarse complejos sistemas de reglas para rellenar estos ``huecos'' haciendo que las plantillas sean difíciles de distinguir de otros métodos más sofisticados \citep{deemter2005real}. Las desventajas de esta aproximación son que se caracterizan por resultar excesivamente laboriosas si se construyen a mano, aunque se han desarrollado investigaciones para su generación automática a partir de corpus como la de \cite{angeli2012parsing} y la de \cite{kondadadi2013statistical}; y que no son una solución escalable para aplicaciones que requieran variaciones lingüísticas considerables.

Una alternativa a las plantillas son los sistemas en la gramática. Dichas gramáticas pueden ser construidas manualmente \citep{elhadad1996overview} o mediante inferencia \citep{parekh2000grammar} a través de corpus. La mayor dificultad de los sistemas basados en gramática es cómo tomar decisiones entre varias opciones válidas y relacionadas entre sí. La otra alternativa son los métodos estadísticos, en los que se pueden encontrar el aprovechamiento de técnicas como la gramática categórica combinatoria \citep{steedman2000}, como en el trabajo de \cite{espinosa2008hypertagging}, el método de construcción de árbol adyacente a la gramática \citep{gardent2015multiple} y todo tipo de técnicas de aprendizaje automático como las máquinas de soporte de vectores \citep{bohnet2010broad}, entre otros.

La última tarea es la denominada realización estructural. En ella se espera convertir las estructuras abstractas, como párrafos y secciones, en símbolos de marcado entendidos por el componente de presentación del documento (como por ejemplo las directivas HTML si se va a presentar en formato web). Algunos autores como \cite{gatt2018survey}, no consideran esta fase como parte de la arquitectura realizer. En cualquier caso, la salida del módulo de realización será el texto final generado.

En resumen, la arquitectura realizer consta de tres módulos: macro planificación, micro planificación y realización. El primero de ellos recibe una entrada en forma de tupla que incluye la base de conocimiento, el propósito comunicativo, el modelo de usuario y la historia del discurso. Con estos cuatro elementos, lleva a cabo las tareas de determinación del contenido y estructuración del documento, generando el plan del documento, que será la entra del módulo de micro planificación. Este último será el encargado de la agregación, lexicalización y la generación de expresiones referenciales, produciendo lo que se conoce como especificación del texto. Dicha entidad será recibida por el módulo de realización y ejecutará la realización lingüística y la realización estructural (cuando sea necesario), generando así el texto de lenguaje natural que se espera del sistema.

\subsubsection{Arquitectura transformer}\label{sss:transformer}
Como se ha señalado anteriormente, una de las grandes desventajas que presenta la arquitectura realizer, es su gigantesca dependencia del dominio del discurso en muchas de sus componentes. De hecho, la solución propuesta para implementar la determinación del contenido es una estructura de datos construida ad hoc, y esta es solo la primera fase de todo el pipeline. Esto no supone un problema cuando se plantea el desarrollo de sistemas de generación de lenguaje natural que pueden enmarcarse dentro de un ámbito específico (como los informes meteorológicos, las locuciones deportivas, etcétera). Sin embargo, cuando se busca generar correos electrónicos, el usuario podría plantearse escribir un e-mail sobre diversas temáticas muy variadas que resulta complicado restringir dentro de un campo semántico. Este tipo de problemas, dada su complejidad, suelen abordarse utilizando arquitecturas compuestas por redes neuronales \citep{goldberg2016primer}, normalmente de gran cantidad de capas ocultas (\textit{deep learning} o aprendizaje profundo).