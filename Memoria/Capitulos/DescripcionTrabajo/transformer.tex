\section{Implementación de la arquitectura transformer}\label{s:transformer}

A diferencia de los problemas que se encuentran al tratar de construir una arquitectura realizer para un problema sin dominio específico como es la redacción automática de correos electrónicos, los transformers son capaces de afrontar la generación de lenguaje natural sin la necesidad de enmarcar los textos dentro de un ámbito concreto. De hecho, normalmente se suele acudir a esta arquitectura cuando se pretende hacer frente a problemas en los que podría requerirse un conocimiento general.

Como en el apartado \ref{sss:transformer}, se entró bastante en detalle en la estructura de la arquitectura, cada uno de sus módulos y la filosofía detrás de cada uno de ellos, esta sección se limitará a exponer las vicisitudes específicas del problema que compete a este trabajo, empezando por la entrada de la aplicación.

La entrada del modelo debe ser un conjunto de no más de seis Information Items construidos como tuplas sujeto-verbo-objeto. Sin embargo, esto no coincide del todo con la definición de la entrada del modelo transformer. Para resolverlo, en primer lugar se tomará la lista de todos los InIts y se tokenizarán por separado. La tarea de tokenización se lleva a cabo utilizando un tokenizador preentrenado\footnote{\url{https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer}}.

Con una lista de las tuplas Inits tokenizadas, se concatenan como si constituyeran un solo tensor. Esto podría generar la preocupación de que es necesario antes homogeneizar los tensores asegurándose de que todos los sujetos poseen la misma longitud mediante la técnica de \textit{padding} (todos los tensores se adaptan a la longitud del mayor de ellos rellenando con ceros las últimas dimensiones), y lo mismo con los verbos y objetos. No obstante, además de ser una operación computacionalmente costosa, no es necesario para el correcto funcionamiento de nuestro modelo. El motivo por el que se pueden concatenar los InIts (sobra decir que siempre todos ellos deben seguir el orden sujeto-verbo-objeto, ya que esto sí podría dificultar el entrenamiento y confundir al modelo) y luego hacer padding es que, como se explicó en la sección \ref{sss:transformer}, existen tokens especiales de inicio y fin. Estos se añaden a cada sujeto, verbo y objeto por separado, quedando así todos los elementos claramente delimitados. Es decir, la red neuronal aprenderá a distinguir dónde acaba un elemento de la tupla InIt y dónde comienza el siguiente sin necesidad de incluir ceros entre ellos. Este hecho, no solo ahorra tiempo computacionalmente hablando, sino que también ahorra en memoria, pues los vectores tokenizados de InIts son notablemente más pequeños, y, al reducir el tamaño de los vectores de Information Items, disminuye la entrada de la red y, por ende, el número de parámetros a entrenar.

Tras construir la tokenización de los Information Items, se tokeniza, con el mismo tokenizador preentrenado, el correo electrónico ``resultante''. Así ya se cuenta con la entrada, el tensor concatenado de InIts, y la salida de la red, el tensor del cuerpo del mensaje tokenizado. De esta manera, se está en disposición de entrenar el modelo y evaluar los resultados obtenidos.